{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-24T19:18:49.796898800Z",
     "start_time": "2025-11-24T19:18:47.997001900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SISTEMA DE PROCESAMIENTO DE TEXTO ===\n",
      "============================================================\n",
      "\n",
      "1. EMAILS DETECTADOS POR DOCUMENTO:\n",
      "----------------------------------------\n",
      "üìß tecnologia: ['soporte@techcompany.com']\n",
      "üìß salud: ['estudios@hospitalmoderno.org']\n",
      "üìß educacion: ['info@academiadigital.edu']\n",
      "\n",
      "2. T√âRMINOS M√ÅS RELEVANTES POR DOCUMENTO (Top 5):\n",
      "-------------------------------------------------------\n",
      "\n",
      "üìÑ DOCUMENTO: tecnologia\n",
      "T√©rminos m√°s relevantes:\n",
      "   1. learning: 0.0345\n",
      "   2. datar: 0.0345\n",
      "   3. machine: 0.0172\n",
      "   4. contactar_computaci√≥n: 0.0172\n",
      "   5. transformar_industria: 0.0172\n",
      "\n",
      "üìÑ DOCUMENTO: salud\n",
      "T√©rminos m√°s relevantes:\n",
      "   1. importante: 0.0185\n",
      "   2. estudios@hospitalmoderno.org: 0.0185\n",
      "   3. enfermedad: 0.0185\n",
      "   4. prevenir: 0.0185\n",
      "   5. regular: 0.0185\n",
      "\n",
      "üìÑ DOCUMENTO: educacion\n",
      "T√©rminos m√°s relevantes:\n",
      "   1. info@academiadigital.edu: 0.0185\n",
      "   2. curso: 0.0185\n",
      "   3. futuro: 0.0185\n",
      "   4. crucial: 0.0185\n",
      "   5. digital: 0.0185\n",
      "\n",
      "3. MATRIZ T√âRMINO-DOCUMENTO COMPLETA:\n",
      "----------------------------------------\n",
      "Dimensiones: 162 t√©rminos √ó 3 documentos\n",
      "\n",
      "Matriz (primeras 15 filas):\n",
      "                tecnologia  salud  educacion\n",
      "machine             0.0172    0.0     0.0000\n",
      "learning            0.0345    0.0     0.0000\n",
      "inteligencia        0.0172    0.0     0.0000\n",
      "artificial          0.0172    0.0     0.0000\n",
      "transformar         0.0172    0.0     0.0000\n",
      "industria           0.0172    0.0     0.0000\n",
      "algoritmo           0.0172    0.0     0.0000\n",
      "deep                0.0172    0.0     0.0000\n",
      "permitir            0.0172    0.0     0.0185\n",
      "reconocimiento      0.0172    0.0     0.0000\n",
      "imagen              0.0172    0.0     0.0000\n",
      "avanzado            0.0172    0.0     0.0000\n",
      "consulta            0.0172    0.0     0.0000\n",
      "t√©cnico             0.0172    0.0     0.0000\n",
      "contactar           0.0172    0.0     0.0000\n",
      "\n",
      "4. ESTAD√çSTICAS DEL SISTEMA:\n",
      "------------------------------\n",
      "Total de documentos procesados: 3\n",
      "Total de t√©rminos √∫nicos: 162\n",
      "Total de emails detectados: 3\n",
      "\n",
      "============================================================\n",
      "INFORMACI√ìN ADICIONAL PARA AN√ÅLISIS:\n",
      "============================================================\n",
      "\n",
      "Ejemplos de bigramas generados:\n",
      "\n",
      "tecnologia: ['machine_learning', 'learning_inteligencia', 'inteligencia_artificial', 'artificial_transformar', 'transformar_industria']...\n",
      "\n",
      "salud: ['medicina_preventivo', 'preventivo_avance', 'avance_telemedicina', 'telemedicina_mejorar', 'mejorar_calidad']...\n",
      "\n",
      "educacion: ['educaci√≥n_online', 'online_revolucionar', 'revolucionar_acceso', 'acceso_conocimiento', 'conocimiento_global']...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import nltk\n",
    "\n",
    "# Descargar stopwords si no est√°n disponibles\n",
    "try:\n",
    "    nltk_stopwords.words('spanish')\n",
    "    nltk_stopwords.words('english')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "class ProcesadorAvanzado:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "        self.stopwords_es = set(nltk_stopwords.words('spanish'))\n",
    "        self.stopwords_en = set(nltk_stopwords.words('english'))\n",
    "        self.stopwords = self.stopwords_es.union(self.stopwords_en)\n",
    "\n",
    "    def extraer_emails(self, texto):\n",
    "        patron = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        return re.findall(patron, texto)\n",
    "\n",
    "    def limpiar_y_lematizar(self, texto):\n",
    "        # Extraer emails primero y preservarlos\n",
    "        emails = self.extraer_emails(texto)\n",
    "\n",
    "        # Remover emails del texto para procesamiento normal\n",
    "        texto_sin_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', texto)\n",
    "\n",
    "        # Procesar con spaCy\n",
    "        doc = self.nlp(texto_sin_emails.lower())\n",
    "\n",
    "        # Lematizar y filtrar\n",
    "        tokens_lematizados = []\n",
    "        for token in doc:\n",
    "            if (token.is_alpha and\n",
    "                    not token.is_stop and\n",
    "                    token.text not in self.stopwords and\n",
    "                    len(token.text) > 2):\n",
    "                tokens_lematizados.append(token.lemma_)\n",
    "\n",
    "        return tokens_lematizados, emails\n",
    "\n",
    "    def generar_bigramas(self, tokens):\n",
    "        return ['_'.join(bg) for bg in ngrams(tokens, 2)]\n",
    "\n",
    "    def generar_trigramas(self, tokens):\n",
    "        return ['_'.join(tg) for tg in ngrams(tokens, 3)]\n",
    "\n",
    "\n",
    "class ModeloEspacioVectorial:\n",
    "    def __init__(self):\n",
    "        self.documentos = {}\n",
    "        self.matriz_tf = None\n",
    "        self.terminos = set()\n",
    "        self.nombres_docs = []\n",
    "\n",
    "    def agregar_documento(self, nombre, tokens):\n",
    "        self.documentos[nombre] = tokens\n",
    "        self.terminos.update(tokens)\n",
    "        self.nombres_docs.append(nombre)\n",
    "\n",
    "    def calcular_tf(self, tokens):\n",
    "        total_terminos = len(tokens)\n",
    "        frecuencias = Counter(tokens)\n",
    "\n",
    "        tf_normalizado = {}\n",
    "        for termino, freq in frecuencias.items():\n",
    "            tf_normalizado[termino] = freq / total_terminos\n",
    "\n",
    "        return tf_normalizado\n",
    "\n",
    "    def construir_matriz(self):\n",
    "        if not self.documentos:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        matriz_data = {}\n",
    "\n",
    "        for doc_name, tokens in self.documentos.items():\n",
    "            tf_doc = self.calcular_tf(tokens)\n",
    "            matriz_data[doc_name] = tf_doc\n",
    "\n",
    "        self.matriz_tf = pd.DataFrame(matriz_data).fillna(0)\n",
    "        self.matriz_tf = self.matriz_tf[self.nombres_docs]\n",
    "\n",
    "        return self.matriz_tf\n",
    "\n",
    "    def obtener_terminos_relevantes(self, doc_index, top_n=5):\n",
    "        if self.matriz_tf is None:\n",
    "            self.construir_matriz()\n",
    "\n",
    "        if doc_index >= len(self.nombres_docs):\n",
    "            raise ValueError(\"√çndice de documento fuera de rango\")\n",
    "\n",
    "        doc_name = self.nombres_docs[doc_index]\n",
    "\n",
    "        terminos_relevantes = (\n",
    "            self.matriz_tf[doc_name]\n",
    "            .sort_values(ascending=False)\n",
    "            .head(top_n)\n",
    "        )\n",
    "\n",
    "        return terminos_relevantes\n",
    "\n",
    "\n",
    "class SistemaProcesamientoTexto:\n",
    "    def __init__(self):\n",
    "        self.procesador = ProcesadorAvanzado()\n",
    "        self.modelo = ModeloEspacioVectorial()\n",
    "        self.documentos_originales = {}\n",
    "        self.emails_por_documento = {}\n",
    "\n",
    "    def agregar_documento(self, nombre, texto):\n",
    "        \"\"\"Agrega un documento al sistema\"\"\"\n",
    "        self.documentos_originales[nombre] = texto\n",
    "\n",
    "        # Procesar el texto\n",
    "        tokens_lematizados, emails = self.procesador.limpiar_y_lematizar(texto)\n",
    "\n",
    "        # Generar bigramas\n",
    "        bigramas = self.procesador.generar_bigramas(tokens_lematizados)\n",
    "\n",
    "        # Combinar tokens simples y bigramas\n",
    "        tokens_completos = tokens_lematizados + bigramas\n",
    "\n",
    "        # Agregar emails como t√©rminos especiales\n",
    "        tokens_completos.extend(emails)\n",
    "\n",
    "        # Guardar emails para reporte\n",
    "        self.emails_por_documento[nombre] = emails\n",
    "\n",
    "        # Agregar al modelo vectorial\n",
    "        self.modelo.agregar_documento(nombre, tokens_completos)\n",
    "\n",
    "    def generar_reporte(self):\n",
    "        \"\"\"Genera el reporte completo del sistema\"\"\"\n",
    "        print(\"=== SISTEMA DE PROCESAMIENTO DE TEXTO ===\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # 1. Mostrar emails detectados\n",
    "        print(\"\\n1. EMAILS DETECTADOS POR DOCUMENTO:\")\n",
    "        print(\"-\" * 40)\n",
    "        for doc_name, emails in self.emails_por_documento.items():\n",
    "            print(f\"üìß {doc_name}: {emails if emails else 'No se detectaron emails'}\")\n",
    "\n",
    "        # 2. Construir matriz y mostrar t√©rminos relevantes\n",
    "        matriz = self.modelo.construir_matriz()\n",
    "\n",
    "        print(f\"\\n2. T√âRMINOS M√ÅS RELEVANTES POR DOCUMENTO (Top 5):\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "        for i, doc_name in enumerate(self.modelo.nombres_docs):\n",
    "            print(f\"\\nüìÑ DOCUMENTO: {doc_name}\")\n",
    "            print(\"T√©rminos m√°s relevantes:\")\n",
    "            terminos_relevantes = self.modelo.obtener_terminos_relevantes(i, 5)\n",
    "\n",
    "            for j, (termino, peso) in enumerate(terminos_relevantes.items(), 1):\n",
    "                print(f\"   {j}. {termino}: {peso:.4f}\")\n",
    "\n",
    "        # 3. Mostrar matriz completa\n",
    "        print(f\"\\n3. MATRIZ T√âRMINO-DOCUMENTO COMPLETA:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Dimensiones: {matriz.shape[0]} t√©rminos √ó {matriz.shape[1]} documentos\")\n",
    "        print(\"\\nMatriz (primeras 15 filas):\")\n",
    "        print(matriz.head(15).round(4))\n",
    "\n",
    "        # 4. Estad√≠sticas generales\n",
    "        print(f\"\\n4. ESTAD√çSTICAS DEL SISTEMA:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Total de documentos procesados: {len(self.documentos_originales)}\")\n",
    "        print(f\"Total de t√©rminos √∫nicos: {len(self.modelo.terminos)}\")\n",
    "        print(f\"Total de emails detectados: {sum(len(emails) for emails in self.emails_por_documento.values())}\")\n",
    "\n",
    "        return {\n",
    "            'matriz': matriz,\n",
    "            'terminos_relevantes': {\n",
    "                doc: self.modelo.obtener_terminos_relevantes(i, 5)\n",
    "                for i, doc in enumerate(self.modelo.nombres_docs)\n",
    "            },\n",
    "            'emails': self.emails_por_documento\n",
    "        }\n",
    "\n",
    "\n",
    "# CASO DE PRUEBA\n",
    "if __name__ == \"__main__\":\n",
    "    # Crear sistema\n",
    "    sistema = SistemaProcesamientoTexto()\n",
    "\n",
    "    # Documentos de prueba sobre diferentes temas\n",
    "    documentos = {\n",
    "        \"tecnologia\": \"\"\"\n",
    "        El machine learning y la inteligencia artificial est√°n transformando la industria. \n",
    "        Los algoritmos de deep learning permiten reconocimiento de im√°genes avanzado. \n",
    "        Para consultas t√©cnicas contactar a: soporte@techcompany.com \n",
    "        La computaci√≥n en la nube y big data son esenciales para el an√°lisis de datos.\n",
    "        Python se ha convertido en el lenguaje preferido para data science.\n",
    "        \"\"\",\n",
    "\n",
    "        \"salud\": \"\"\"\n",
    "        La medicina preventiva y los avances en telemedicina mejoran la calidad de vida. \n",
    "        Investigadores en gen√≥mica estudian terapias personalizadas contra el c√°ncer.\n",
    "        Contacto para estudios cl√≠nicos: estudios@hospitalmoderno.org\n",
    "        La nutrici√≥n balanceada y ejercicio regular previenen enfermedades cardiovasculares.\n",
    "        La salud mental es igual de importante que la f√≠sica.\n",
    "        \"\"\",\n",
    "\n",
    "        \"educacion\": \"\"\"\n",
    "        La educaci√≥n online ha revolucionado el acceso al conocimiento global. \n",
    "        Las plataformas de e-learning permiten aprendizaje personalizado adaptado a cada estudiante.\n",
    "        Informaci√≥n sobre cursos: info@academiadigital.edu\n",
    "        La gamificaci√≥n y realidad aumentada crean experiencias educativas inmersivas.\n",
    "        El desarrollo de habilidades digitales es crucial para el futuro laboral.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # Procesar documentos\n",
    "    for nombre, texto in documentos.items():\n",
    "        sistema.agregar_documento(nombre, texto)\n",
    "\n",
    "    # Generar reporte completo\n",
    "    reporte = sistema.generar_reporte()\n",
    "\n",
    "    # Informaci√≥n adicional para an√°lisis\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INFORMACI√ìN ADICIONAL PARA AN√ÅLISIS:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Mostrar algunos bigramas detectados\n",
    "    print(\"\\nEjemplos de bigramas generados:\")\n",
    "    for doc_name in documentos.keys():\n",
    "        texto = documentos[doc_name]\n",
    "        tokens, _ = sistema.procesador.limpiar_y_lematizar(texto)\n",
    "        bigramas = sistema.procesador.generar_bigramas(tokens)\n",
    "        print(f\"\\n{doc_name}: {bigramas[:5]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d0783b08d84682"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
